{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T09:12:59.858803Z",
     "start_time": "2025-06-11T09:12:56.175509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg # For eigvals and lstsq\n",
    "import time\n",
    "\n",
    "# Załaduj dane (jak w Twoim kodzie)\n",
    "# Pamiętaj, że 'labels' to nazwy kolumn z pliku labels\n",
    "with open(\"dataset/breast-cancer.labels\", \"r\") as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "train_data = pd.io.parsers.read_csv(\"dataset/breast-cancer-train.dat\", names=labels)\n",
    "validate_data = pd.io.parsers.read_csv(\"dataset/breast-cancer-validate.dat\", names=labels)\n",
    "\n",
    "# Przekształć kolumnę \"Malignant/Benign\" na wartości liczbowe (1 dla M, -1 dla B)\n",
    "# To jest spójne z Twoją implementacją least_square\n",
    "train_data['Malignant/Benign_numeric'] = np.where(train_data['Malignant/Benign'] == \"M\", 1, -1)\n",
    "validate_data['Malignant/Benign_numeric'] = np.where(validate_data['Malignant/Benign'] == \"M\", 1, -1)\n",
    "\n",
    "\n",
    "def gradient_descent_for_linear_regression(X_train, y_train, X_validate, y_validate, alpha, n_iterations, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Implementacja Gradient Descent dla Liniowej Regresji (z zastosowaniem do klasyfikacji).\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Macierz cech treningowych (z dodaną kolumną biasu).\n",
    "        y_train (np.ndarray): Wektor etykiet treningowych (-1 lub 1).\n",
    "        X_validate (np.ndarray): Macierz cech walidacyjnych (z dodaną kolumną biasu).\n",
    "        y_validate (np.ndarray): Wektor etykiet walidacyjnych (-1 lub 1).\n",
    "        alpha (float): Stała ucząca (learning rate).\n",
    "        n_iterations (int): Liczba iteracji algorytmu Gradient Descent.\n",
    "        threshold (float): Próg klasyfikacji (domyślnie 0.0 dla -1/1 etykiet).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (theta, costs, tp, tn, fp, fn, accuracy)\n",
    "               theta (np.ndarray): Ostateczne wagi modelu.\n",
    "               costs (list): Lista wartości funkcji kosztu w kolejnych iteracjach.\n",
    "               tp, tn, fp, fn (int): Liczby True Positives, True Negatives, False Positives, False Negatives.\n",
    "               accuracy (float): Dokładność na zbiorze walidacyjnym.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X_train.shape\n",
    "    theta = np.zeros(n_features) # Inicjalizacja wag zerami\n",
    "    costs = []\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        # Obliczenie predykcji (liniowej)\n",
    "        predictions = X_train @ theta\n",
    "\n",
    "        # Obliczenie błędu\n",
    "        error = predictions - y_train.flatten() # Upewnij się, że y_train jest jednowymiarowy\n",
    "\n",
    "\n",
    "        # Obliczenie gradientu funkcji kosztu (MSE)\n",
    "        # Gradient MSE = (2/n_samples) * X.T @ (X @ theta - y)\n",
    "        # Tutaj pomijamy 2/n_samples bo to tylko stała i zostanie wchłonięta przez alpha\n",
    "        # Albo uwzględniamy ją w gradiencie:\n",
    "        gradient = (2/n_samples) * X_train.T @ error\n",
    "\n",
    "\n",
    "        # Aktualizacja wag\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "        # Obliczenie kosztu (MSE) dla monitorowania\n",
    "        cost = np.mean(error**2) # MSE\n",
    "        costs.append(cost)\n",
    "\n",
    "        # Opcjonalnie: wyświetlanie kosztu co N iteracji\n",
    "        # if iteration % (n_iterations // 10) == 0:\n",
    "        #     print(f\"Iteration {iteration}, Cost: {cost:.4f}\")\n",
    "\n",
    "    # Ocena modelu na zbiorze walidacyjnym\n",
    "    predictions_validate = X_validate @ theta\n",
    "\n",
    "    # Próg klasyfikacji\n",
    "    classified_predictions = np.where(predictions_validate >= threshold, 1, -1)\n",
    "\n",
    "    # Funkcja do obliczenia metryk (skopiowana z Twojego kodu)\n",
    "    def calc_acc(p_vec, b_vec):\n",
    "        tp = np.sum([1 for p, b in zip(p_vec, b_vec) if p > 0 and b > 0]) # p>0 -> klasyfikujemy na M (1)\n",
    "        tn = np.sum([1 for p, b in zip(p_vec, b_vec) if p <= 0 and b < 0]) # p<=0 -> klasyfikujemy na B (-1)\n",
    "        fp = np.sum([1 for p, b in zip(p_vec, b_vec) if p > 0 and b <= 0])\n",
    "        fn = np.sum([1 for p, b in zip(p_vec, b_vec) if p <= 0 and b > 0])\n",
    "        total = tp + tn + fp + fn\n",
    "        accuracy = float((tp + tn) / total) if total > 0 else 0.0\n",
    "        return int(tp), int(tn), int(fp), int(fn), accuracy\n",
    "\n",
    "    tp, tn, fp, fn, accuracy = calc_acc(classified_predictions, y_validate.flatten())\n",
    "\n",
    "    return theta, costs, tp, tn, fp, fn, accuracy\n",
    "\n",
    "\n",
    "def gradient_descent_runner():\n",
    "    # Przygotowanie danych treningowych\n",
    "    # Usuń kolumny \"patient ID\" i oryginalną \"Malignant/Benign\"\n",
    "    A = train_data.drop([\"patient ID\", \"Malignant/Benign\"], axis=1).values\n",
    "    # Dodaj kolumnę jedynek dla biasu (intercept term)\n",
    "    A = np.c_[np.ones(A.shape[0]), A]\n",
    "\n",
    "    # Przygotowanie danych walidacyjnych\n",
    "    A_validate = validate_data.drop([\"patient ID\", \"Malignant/Benign\"], axis=1).values\n",
    "    A_validate = np.c_[np.ones(A_validate.shape[0]), A_validate]\n",
    "\n",
    "    # Etykiety\n",
    "    b = train_data['Malignant/Benign_numeric'].values.reshape(-1, 1)\n",
    "    b_validate = validate_data['Malignant/Benign_numeric'].values.reshape(-1, 1)\n",
    "\n",
    "    # Obliczenie A.T @ A dla wyznaczenia stałej uczącej\n",
    "    # Tutaj A to już macierz z dodanym biasem\n",
    "    AtA = A.T @ A\n",
    "\n",
    "    # Obliczenie wartości własnych\n",
    "    eigv = scipy.linalg.eigvals(AtA)\n",
    "    # Zwróć uwagę, że eigvals zwraca wartości zespolone, musimy wziąć część rzeczywistą\n",
    "    real_eigv = np.real(eigv)\n",
    "\n",
    "    # Wyznaczenie stałej uczącej alpha\n",
    "    # Dla stabilności i zbieżności często używa się 1 / (lambda_max) lub 2 / (lambda_max + lambda_min)\n",
    "    # Zadanie mówi \"najmniejszej i największej wartości własnej macierzy A^T A\"\n",
    "    # Dwie popularne formuły to:\n",
    "    # 1. alpha = 1 / lambda_max\n",
    "    # 2. alpha = 2 / (lambda_max + lambda_min) (dla optymalnego zbiegania w najgorszym przypadku)\n",
    "    # Wybieram 1 / lambda_max, jako prostszą i często działającą, ale 2/(l_max+l_min) jest teoretycznie lepsze dla niektórych f-cji\n",
    "    # Pamiętaj, że lambda_min może być bliskie 0, co może spowodować dużą alphę\n",
    "    # Jeśli min_eigv jest bardzo małe, lepiej użyć 1/max_eigv\n",
    "    lambda_max = np.max(real_eigv)\n",
    "    lambda_min = np.min(real_eigv)\n",
    "\n",
    "    # Dobór alpha:\n",
    "    # Jeśli lambda_min jest bliskie 0, 1/(max+min) może być niestabilne\n",
    "    # Stosuje się również 1/max. Możesz eksperymentować.\n",
    "    # Upewnij się, że lambda_max jest dodatnie!\n",
    "    if lambda_max > 0:\n",
    "        alpha = 1 / lambda_max\n",
    "        # Możesz zmniejszyć alpha nieznacznie, np. alpha = 0.99 / lambda_max, aby zwiększyć stabilność\n",
    "        # alpha = 0.99 * alpha\n",
    "    else:\n",
    "        print(\"Warning: lambda_max is not positive. Defaulting alpha to a small value.\")\n",
    "        alpha = 0.001 # Wartość awaryjna\n",
    "\n",
    "    print(f\"Najmniejsza wartość własna A.T @ A: {lambda_min:.4f}\")\n",
    "    print(f\"Największa wartość własna A.T @ A: {lambda_max:.4f}\")\n",
    "    print(f\"Wyliczona stała ucząca (alpha): {alpha:.6f}\")\n",
    "\n",
    "    n_iterations = 100000 # Liczba iteracji, możesz ją dostosować\n",
    "\n",
    "    start_time_gd = time.time()\n",
    "    theta_gd, costs_gd, tp_gd, tn_gd, fp_gd, fn_gd, acc_gd = gradient_descent_for_linear_regression(\n",
    "        A, b, A_validate, b_validate, alpha, n_iterations\n",
    "    )\n",
    "    end_time_gd = time.time()\n",
    "    time_gd = end_time_gd - start_time_gd\n",
    "\n",
    "    print(\"\\n--- Wyniki Gradient Descent ---\")\n",
    "    print(f\"Wagi (theta_gd): {theta_gd}\")\n",
    "    print(f\"Czas obliczeń (GD): {time_gd:.4f} s\")\n",
    "    print(f\"TP: {tp_gd}, TN: {tn_gd}, FP: {fp_gd}, FN: {fn_gd}\")\n",
    "    print(f\"Dokładność na zbiorze walidacyjnym (GD): {acc_gd:.4f}\")\n",
    "\n",
    "    return (tp_gd, tn_gd, fp_gd, fn_gd, acc_gd), time_gd\n",
    "\n",
    "# Teraz możesz uruchomić obie funkcje i porównać wyniki\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Rozpoczynam Gradient Descent...\")\n",
    "    gd_results, gd_time = gradient_descent_runner()\n",
    "\n",
    "    print(\"\\n--- Podsumowanie Porównania ---\")\n",
    "    print(f\"Metoda:                 | Dokładność | Czas (s) | Złożoność Teoretyczna\")\n",
    "    print(f\"------------------------|------------|----------|-----------------------\")\n",
    "    print(f\"Gradient Descent        | {gd_results[4]:.4f}   | {gd_time:.4f}  | O(iterations * m * d)\")\n",
    "    print(\"\\n(gdzie m = liczba_próbek, d = liczba_cech)\")\n",
    "\n",
    "    # Możesz tutaj dodać bardziej szczegółowe porównanie np. wykres kosztu GD\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.plot(gd_costs)\n",
    "    # plt.xlabel(\"Iteration\")\n",
    "    # plt.ylabel(\"Cost\")\n",
    "    # plt.title(\"Gradient Descent Cost over Iterations\")\n",
    "    # plt.show()"
   ],
   "id": "eb0e05bfdbeb3feb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam Gradient Descent...\n",
      "Najmniejsza wartość własna A.T @ A: 0.0003\n",
      "Największa wartość własna A.T @ A: 545289637.0781\n",
      "Wyliczona stała ucząca (alpha): 0.000000\n",
      "\n",
      "--- Wyniki Gradient Descent ---\n",
      "Wagi (theta_gd): [-7.42051860e-05 -5.31352492e-04 -9.21711928e-04 -3.15476015e-03\n",
      " -1.55449841e-03 -5.90130406e-06  1.74146533e-07  5.46860157e-06\n",
      "  2.84526799e-06 -1.12310824e-05 -4.54034645e-06 -3.22435765e-06\n",
      " -8.87580830e-05  5.88519777e-07  1.08467466e-03 -5.42416358e-07\n",
      " -4.77966426e-07 -8.35640185e-07 -3.09457665e-07 -1.50651104e-06\n",
      " -2.37043947e-07 -5.15357257e-04 -1.14153215e-03 -2.97505261e-03\n",
      "  1.95150790e-03 -7.40356043e-06  5.88963155e-06  1.16338156e-05\n",
      "  3.40693920e-06 -1.42680946e-05 -4.35887013e-06  2.76188046e-04]\n",
      "Czas obliczeń (GD): 3.4509 s\n",
      "TP: 49, TN: 193, FP: 7, FN: 11\n",
      "Dokładność na zbiorze walidacyjnym (GD): 0.9308\n",
      "\n",
      "--- Podsumowanie Porównania ---\n",
      "Metoda:                 | Dokładność | Czas (s) | Złożoność Teoretyczna\n",
      "------------------------|------------|----------|-----------------------\n",
      "Gradient Descent        | 0.9308   | 3.4509  | O(iterations * m * d)\n",
      "\n",
      "(gdzie m = liczba_próbek, d = liczba_cech)\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
